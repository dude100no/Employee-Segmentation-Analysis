{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "Name: Koh June Wen\n",
    "\n",
    "Admin Number: 2112956\n",
    "\n",
    "Class: DAAAFT2A04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv(\"Company_Employee.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Resign Status'] == 'Yes'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,3, figsize=(20, 10))\n",
    "sns.countplot(data=df, x='Gender', ax=ax[0,0])\n",
    "sns.countplot(data=df, x='BusinessTravel', ax=ax[0, 1])\n",
    "sns.countplot(data=df, x='Job Function', ax=ax[0,2])\n",
    "sns.countplot(data=df, x='MaritalStatus', ax=ax[1,0])\n",
    "sns.countplot(data=df, x='Resign Status', ax=ax[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,square=True, linewidth=.5, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "## shows that there is little or not correlation between the features of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x='Salary ($)', y='Age', hue='Job Function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.histplot(data=df, x='Length of Service (Years)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "1. Use `OrdinalEncoder`. You transform categorical feature to just one column. The problem may be that the difference between 2 categories may be different from another combination which may not be accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinal Encoding\n",
    "Some of the features have ordinal values but in categorical values.\\\n",
    "E.g. BusinessTravel feature has 'Non travel', 'Travel Rarely' and 'Travel Frequently'\n",
    "the difference between 'Non travel' and 'Travel Frequently' is evidently greater than the difference between 'Non travel' and 'Travel Rarely'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oe = OrdinalEncoder(categories=[['Non-Travel', 'Travel_Rarely', 'Travel_Frequently']])\n",
    "# oe.fit(ordinal)\n",
    "\n",
    "# oe.transform(df)\n",
    "\n",
    "oe_businesstravel = oe.fit_transform(df[['BusinessTravel']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oe_weighted = OrdinalEncoder(categories=[['Non-Travel', 'Travel_Rarely', '', 'Travel_Frequently']])\n",
    "oe_weighted_businesstravel = oe_weighted.fit_transform(df[['BusinessTravel']])\n",
    "oe_weighted_businesstravel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gower Dissimilarity\n",
    "\n",
    "Gower Distance is a distance measure that can be used to calculate distance between two entity whose attributes has a mix categorical and numerical values. The distance is always a number between 0 (identical) and 1 (maximally dissimilar)\n",
    "\n",
    "`Quantitative (interval)`: range-normalized (Manhattan distance)\\\n",
    "`Ordinal`: variable is first ranked, then Manhattan distance is used with a special adjustment for ties\\\n",
    "`Nominal`: variable of <i>k</i> categories are first converted into <i>k</i> binary columns and then the Dice coefficient is used\n",
    "\n",
    "Gower Dissimilarity is non-Euclidean and non-metric. However, Gower Dissimilarity is actually Euclidean distance when no specially processed ordinal variables are used\n",
    "\n",
    "As we are using a distance that is not obeying the Euclidean geometry, methods based on Euclidean distance must not be used e.g. K-means, Ward, etc.\n",
    "\n",
    "This method is mainly used to deal with the many categorical variables in the dataset. The sample space for categorical data is discrete, and doesn't have a natural origin. A Euclidean distance function on such a space isn't really meaningful. As someone put it, \"The fact that a snake possesses neither wheels nor legs allows us to say nothing about the relative value of wheels and legs.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gower_df = gower.gower_matrix(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gower_resign_df = gower.gower_matrix(df[df['Resign Status'] == 'Yes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oe_df = df.copy()\n",
    "oe_df['BusinessTravel'] = oe_businesstravel\n",
    "\n",
    "gower_oe_df = gower.gower_matrix(oe_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oe_weighted_df = df.copy()\n",
    "oe_weighted_df['BusinessTravel'] = oe_weighted_businesstravel\n",
    "\n",
    "gower_oe_weighted_df = gower.gower_matrix(oe_weighted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oe_weighted_df = df.copy()\n",
    "oe_weighted_df['BusinessTravel'] = oe_weighted_businesstravel\n",
    "oe_weighted_male_df = oe_weighted_df[oe_weighted_df['Gender'] == 'Male']\n",
    "oe_weighted_female_df = oe_weighted_df[oe_weighted_df['Gender'] == 'Female']\n",
    "oe_weighted_resigned_df = oe_weighted_df[oe_weighted_df['Resign Status'] == 'Yes']\n",
    "oe_weighted_notresigned_df = oe_weighted_df[oe_weighted_df['Resign Status'] == 'No']\n",
    "\n",
    "gower_oe_weighted_male_df = gower.gower_matrix(oe_weighted_male_df)\n",
    "gower_oe_weighted_female_df = gower.gower_matrix(oe_weighted_female_df)\n",
    "gower_oe_weighted_resigned_df = gower.gower_matrix(oe_weighted_resigned_df)\n",
    "gower_oe_weighted_notresigned_df = gower.gower_matrix(oe_weighted_notresigned_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling only numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_num = scaler.fit_transform(df[['Age', 'Distance Between Company and Home (KM)', 'Salary ($)', 'Length of Service (Years)']][df['Resign Status'] == 'Yes'])\n",
    "scaled_num_df = pd.DataFrame(scaled_num, columns=['Age', 'Distance Between Company and Home (KM)', 'Salary ($)', 'Length of Service (Years)'])\n",
    "scaled_num_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OHE\n",
    "\n",
    "You will transform categorical feature to four new columns, where will be just one 1 and other 0. The problem here is that difference between 2 combinations of categories will be the same as the combinations of 2 other categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummying the categoric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dummying the categoric features\n",
    "\n",
    "categories = ['Gender', 'BusinessTravel', 'Job Function', 'MaritalStatus', 'Resign Status']\n",
    "\n",
    "ohc = OneHotEncoder(sparse=False, drop='first', handle_unknown='ignore')\n",
    "c_transformed_df = ohc.fit_transform(df[categories])\n",
    "dummied_categories_df = pd.DataFrame(c_transformed_df, columns=ohc.get_feature_names_out())\n",
    "new_df = pd.concat([dummied_categories_df, df.drop(columns=categories)], axis=1)\n",
    "\n",
    "## Scaling the data\n",
    "scaler = StandardScaler()\n",
    "scaled_df = scaler.fit_transform(new_df)\n",
    "new_df_scaled = pd.DataFrame(scaled_df, columns=new_df.columns)\n",
    "new_df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_scaled.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oe_df = df.copy()\n",
    "oe_df['BusinessTravel'] = oe_businesstravel\n",
    "\n",
    "categories = ['Gender', 'Job Function', 'MaritalStatus', 'Resign Status']\n",
    "ohc = OneHotEncoder(sparse=False, drop='first', handle_unknown='ignore')\n",
    "c_transformed_df = ohc.fit_transform(oe_df[categories])\n",
    "dummied_categories_df = pd.DataFrame(c_transformed_df, columns=ohc.get_feature_names_out())\n",
    "new_df_oe = pd.concat([dummied_categories_df, oe_df.drop(columns=categories)], axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_oe_df = scaler.fit_transform(new_df_oe)\n",
    "new_df_scaled_oe = pd.DataFrame(scaled_oe_df, columns=new_df_oe.columns)\n",
    "new_df_scaled_oe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_scaled_oe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oe_weighted_df = df.copy()\n",
    "oe_weighted_df['BusinessTravel'] = oe_weighted_businesstravel\n",
    "\n",
    "categories = ['Gender', 'Job Function', 'MaritalStatus', 'Resign Status']\n",
    "ohc = OneHotEncoder(sparse=False, drop='first', handle_unknown='ignore')\n",
    "c_transformed_df = ohc.fit_transform(oe_weighted_df[categories])\n",
    "dummied_categories_df = pd.DataFrame(c_transformed_df, columns=ohc.get_feature_names_out())\n",
    "new_df_oe_weighted = pd.concat([dummied_categories_df, oe_weighted_df.drop(columns=categories)], axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_oe_weighted_df = scaler.fit_transform(new_df_oe_weighted)\n",
    "new_df_scaled_oe_weighted = pd.DataFrame(scaled_oe_weighted_df, columns=new_df_oe_weighted.columns)\n",
    "new_df_scaled_oe_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_scaled_oe_weighted.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oe_weighted_nfirst_df = df.copy()\n",
    "oe_weighted_nfirst_df['BusinessTravel'] = oe_weighted_businesstravel\n",
    "\n",
    "categories = ['Gender', 'Job Function', 'MaritalStatus', 'Resign Status']\n",
    "ohc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "c_transformed_df = ohc.fit_transform(oe_weighted_nfirst_df[categories])\n",
    "dummied_categories_df = pd.DataFrame(c_transformed_df, columns=ohc.get_feature_names_out())\n",
    "new_df_oe_weighted_nfirst = pd.concat([dummied_categories_df, oe_weighted_nfirst_df.drop(columns=categories)], axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_oe_weighted_nfirst_df = scaler.fit_transform(new_df_oe_weighted_nfirst)\n",
    "new_df_scaled_oe_weighted_nfirst = pd.DataFrame(scaled_oe_weighted_nfirst_df, columns=new_df_oe_weighted_nfirst.columns)\n",
    "new_df_scaled_oe_weighted_nfirst.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "I will not use PCA for the unsupervised learning because the cumulative explained variance only reaches more than 80% when there are 10 PCs.\\\n",
    "With this many PCs, it will be very difficult to compare the clustering.\\\n",
    "Also, PCA will result in loss of information. If I were to only compare the first 3 PCs, which only have a cumulative explained variance of 0.3625, for the clustering, it will be very unreliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def pca_results(data, pca):\n",
    "    dimensions = [f\"PC {i}\" for i in range(1, len(pca.components_) + 1)]\n",
    "\n",
    "    components = pd.DataFrame(np.round(pca.components_, 4), columns=data.keys())\n",
    "    components.index = dimensions\n",
    "\n",
    "    ev = pca.explained_variance_.reshape(len(pca.components_), 1)\n",
    "    eigenvalues = pd.DataFrame(np.round(ev, 4), columns=['Eigenvalues'])\n",
    "    eigenvalues.index = dimensions\n",
    "\n",
    "    ratios = pca.explained_variance_ratio_.reshape(len(pca.components_), 1)\n",
    "    variance_ratios = pd.DataFrame(np.round(ratios, 4), columns=['Explained Variance'])\n",
    "    variance_ratios.index = dimensions\n",
    "\n",
    "    cum_ratios = pca.explained_variance_ratio_.cumsum().reshape(len(pca.components_), 1)\n",
    "    cum_variance_ratios = pd.DataFrame(np.round(cum_ratios, 4), columns=['Cumulative Explained Variance'])\n",
    "    cum_variance_ratios.index = dimensions\n",
    "\n",
    "    return pd.concat([eigenvalues, variance_ratios, cum_variance_ratios, components], axis=1)\n",
    "\n",
    "\n",
    "def loadingplot(data, pca, pc_plots, width=5, height=5, margin=0.5):\n",
    "    x_pc = pc_plots[0] - 1\n",
    "    y_pc = pc_plots[1] - 1\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(width, height))\n",
    "\n",
    "    x_min = min(pca.components_[x_pc,:].min(), 0) - margin\n",
    "    x_max = max(pca.components_[x_pc,:].max(), 0) + margin\n",
    "    \n",
    "    y_min = min(pca.components_[y_pc,:].min(), 0) - margin\n",
    "    y_max = max(pca.components_[y_pc,:].max(), 0) + margin\n",
    "\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "\n",
    "    text_pos = 0.1\n",
    "\n",
    "    for i, v in enumerate(pca.components_.T):\n",
    "        ax.arrow(0, 0, v[x_pc], v[y_pc], head_width=0.1, head_length=0.1, linewidth=2, color='red')\n",
    "        ax.text(v[x_pc], v[y_pc] + text_pos, data.columns[i], color='black', ha='center', va='center', fontsize=12)\n",
    "\n",
    "    plt.plot([x_min, x_max], [0, 0], color='k', linestyle='--', linewidth=1)\n",
    "    plt.plot([0, 0], [y_min, y_max], color='k', linestyle='--', linewidth=1)\n",
    "\n",
    "    ax.set_xlabel(f\"PC{x_pc + 1}\", fontsize=14)\n",
    "    ax.set_ylabel(f\"PC{y_pc + 1}\", fontsize=14)\n",
    "    ax.set_title(\"Loading plot\", fontsize=14)\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=min(len(new_df_scaled.columns), len(new_df_scaled.index))).fit(new_df_scaled)\n",
    "pca_samples = pca.transform(new_df_scaled)\n",
    "\n",
    "results = pca_results(new_df_scaled, pca)\n",
    "results\n",
    "\n",
    "## Select the first 10 PCs as their cumulative explained variance is more then 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.plot(np.arange(1, len(pca.explained_variance_) + 1), pca.explained_variance_)\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.set(xlabel='Component Number', ylabel='Eigenvalue', title='Screeplot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca2 = PCA(n_components=10).fit(new_df_scaled)\n",
    "scores2 = pca2.transform(new_df_scaled)\n",
    "\n",
    "ax2 = loadingplot(new_df_scaled, pca2, [10,6], width=7, height=7, margin=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores2_df = pd.DataFrame(scores2, columns=[f\"PC{i}\" for i in range(1, 11)])\n",
    "scores2_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building / Evaluation\n",
    "I decided to use Calinski Harabasz Score instead of Silhouette Score.\\\n",
    "As Calinski Harabasz Score does something similar to the Silhouette Score.\\\n",
    "Also, Silhouette Score may does not take into account the size of the cluster.\\\n",
    "E.g. In the tuning of Agglomerative Clustering, the linkage of <i>single</i> gave a very high Silhouette Score, but in actuality, the cluster found was just a few points which would just be identifying outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, AffinityPropagation, SpectralClustering, DBSCAN, Birch, AgglomerativeClustering, MeanShift\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "# attempts to describe how similar a datapoint is to other datapoints in its cluster,\n",
    "# relative to datapoints not in its cluster\n",
    "# It is bounded between -1 and 1. Closer to -1 suggests incorrect clustering,\n",
    "# while closer to +1 shows that each cluster is very dense.\n",
    "# Advantages:\n",
    "# The score is bounded between -1 for incorrect clustering and +1 for highly dense clustering. Scores around zero indicate overlapping clusters.\n",
    "# Disadvantages:\n",
    "# Generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained through DBSCAN\n",
    "\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "# the ratio of the variance of a datapoint compared to points in other clusters, \n",
    "# against the variance compared to points within its cluster.\n",
    "# A high CH index is desireable.\n",
    "# This score is not bounded\n",
    "# Advantages:\n",
    "# The score is higher when clusters are dense and well separated, which relates to a standard concept of a cluster\n",
    "# Disadvantages:\n",
    "# \n",
    "\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans\n",
    "As KMeans uses Euclidean distance to train, due to the high dimensionality of the dataset (16 dimensions), the Euclidean distance becomes inflated (curse of dimensionality)\n",
    "\n",
    "Disadvantages:\\\n",
    "Extremely susceptible to outliers and noise.\\\n",
    "The Algorithem selects the centroids which is just calculated as the mean of all the points, and not a real point of the distribution, the outliers present in any cluster will cause the centroid to distort and it will also cause the SSE to blow up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2)\n",
    "kmeans.fit(scaled_num_df[['Salary ($)', 'Length of Service (Years)']])\n",
    "y_kmeans = kmeans.predict(scaled_num_df[['Salary ($)', 'Length of Service (Years)']])\n",
    "\n",
    "centers = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 2\n",
    "y = 3\n",
    "z = 9\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "# ax = fig.add_subplot(111, projection = '3d')\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# ax.scatter(scaled_num_df[scaled_num_df.columns[x]], scaled_num_df[scaled_num_df.columns[y]], scaled_num_df[scaled_num_df.columns[z]], c = y_kmeans, s=50)\n",
    "# ax.scatter(centers[:, x], centers[:, y], centers[:, z], c='red', s=200)\n",
    "\n",
    "ax.scatter(scaled_num_df[scaled_num_df.columns[x]], scaled_num_df[scaled_num_df.columns[y]], c = y_kmeans, s=50)\n",
    "ax.scatter(centers[:, 0], centers[:, 1], c='red', s=200)\n",
    "\n",
    "ax.set_xlabel(scaled_num_df.columns[x])\n",
    "ax.set_ylabel(scaled_num_df.columns[y])\n",
    "# ax.set_zlabel(scaled_num_df.columns[z])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia_arr = []\n",
    "silhouette_arr =[]\n",
    "\n",
    "x_ = np.arange(2, 15)\n",
    "\n",
    "for k in range(2, 15):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(scaled_num_df)\n",
    "    pred = kmeans.predict(scaled_num_df)\n",
    "    inertia_arr.append(kmeans.inertia_)\n",
    "    silhouette_arr.append(silhouette_score(scaled_num_df, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_, inertia_arr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_, silhouette_arr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affinity Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_model = AffinityPropagation(random_state=5)\n",
    "ap_model.fit(new_df_scaled)\n",
    "pred = ap_model.predict(new_df_scaled)\n",
    "\n",
    "x_ax = 'Salary ($)'\n",
    "y_ax = 'Length of Service (Years)'\n",
    "\n",
    "plt.scatter(new_df_scaled[x_ax], new_df_scaled[y_ax], c=pred, s=50, cmap='viridis')\n",
    "\n",
    "centers = ap_model.cluster_centers_\n",
    "plt.scatter(centers[:, 12], centers[:, 15], c='red', s=200, alpha=0.8)\n",
    "\n",
    "plt.xlabel(x_ax)\n",
    "plt.ylabel(y_ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_score(new_df_scaled, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_model = MeanShift(cluster_all=False)\n",
    "ms_result = ms_model.fit_predict(new_df_scaled)\n",
    "\n",
    "centers = ms_model.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(ms_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 12\n",
    "y = 15\n",
    "z = 9\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection = '3d')\n",
    "\n",
    "ax.scatter(new_df_scaled[new_df_scaled.columns[x]], new_df_scaled[new_df_scaled.columns[y]], new_df_scaled[new_df_scaled.columns[z]], c = ms_result, s=50)\n",
    "ax.scatter(centers[:, x], centers[:, y], centers[:, z], c='red', s=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_model = SpectralClustering(n_clusters=4, assign_labels='discretize')\n",
    "sc_result = sc_model.fit_predict(new_df_scaled)\n",
    "sc_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 12\n",
    "y = 15\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# ax.scatter(new_df_scaled.iloc[:, x], new_df_scaled.iloc[:, y], c=sc_result, s=50)\n",
    "ax.scatter(new_df_scaled[new_df_scaled.columns[x]], new_df_scaled[new_df_scaled.columns[y]], c=sc_result, s=50)\n",
    "ax.set_xlabel(new_df_scaled.columns[x])\n",
    "ax.set_ylabel(new_df_scaled.columns[y])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_score(new_df_scaled, sc_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative Clustering\n",
    "A hierarchical clustering model\\\n",
    "Each point initially starts as a cluster, and slowly the nearest or similar most clusters merge to create one cluster\\\n",
    "We keep on merging the clusters which are nearest or have a high similarity score to one cluster. So, if we define a cut-off or threshold score for the merging we will get multiple clusters instead of a single one.\\\n",
    "E.g. If we say the threshold similarity metrics score is 0.5, it means the algorithm will stop merging the clusters if no two clusters are found with a similarity score less than 0.5, and the number of clusters present at that step will give the final number of clusters that need to be created to the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_arr = {'complete': [], 'single': [], 'average': []}\n",
    "calinski_arr = {'complete': [], 'single': [], 'average': []}\n",
    "x = np.arange(2, 15)\n",
    "\n",
    "for n in tqdm(x): \n",
    "    ac_model_complete = AgglomerativeClustering(n_clusters=n, compute_distances=True, affinity='precomputed', linkage='complete')\n",
    "    pred_ac_complete = ac_model_complete.fit_predict(gower_oe_weighted_df)\n",
    "\n",
    "    silhouette_arr['complete'].append(silhouette_score(gower_oe_weighted_df, pred_ac_complete))\n",
    "    calinski_arr['complete'].append(calinski_harabasz_score(gower_oe_weighted_df, pred_ac_complete))\n",
    "\n",
    "    ac_model_average = AgglomerativeClustering(n_clusters=n, compute_distances=True, affinity='precomputed', linkage='average')\n",
    "    pred_ac_average = ac_model_average.fit_predict(gower_oe_weighted_df)\n",
    "\n",
    "    silhouette_arr['average'].append(silhouette_score(gower_oe_weighted_df, pred_ac_average))\n",
    "    calinski_arr['average'].append(calinski_harabasz_score(gower_oe_weighted_df, pred_ac_average))\n",
    "\n",
    "    ac_model_single = AgglomerativeClustering(n_clusters=n, compute_distances=True, affinity='precomputed', linkage='single')\n",
    "    pred_ac_single = ac_model_single.fit_predict(gower_oe_weighted_df)\n",
    "\n",
    "    silhouette_arr['single'].append(silhouette_score(gower_oe_weighted_df, pred_ac_single))\n",
    "    calinski_arr['single'].append(calinski_harabasz_score(gower_oe_weighted_df, pred_ac_single))\n",
    "    # distances_arr.append(ac_model_single.distances_)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "# silhouette score plot\n",
    "ax1.title.set_text('Silhouette Score')\n",
    "ax1.plot(x, silhouette_arr['complete'], label='complete')\n",
    "ax1.plot(x, silhouette_arr['single'], label='single')\n",
    "ax1.plot(x, silhouette_arr['average'], label='average')\n",
    "\n",
    "# calinski harabas score plot\n",
    "ax2.title.set_text('Calinski Harabas Z Score')\n",
    "ax2.plot(x, calinski_arr['complete'], label='complete')\n",
    "ax2.plot(x, calinski_arr['single'], label='single')\n",
    "ax2.plot(x, calinski_arr['average'], label='average')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_model = AgglomerativeClustering(n_clusters=2, compute_distances=True, affinity='precomputed', linkage='complete')\n",
    "ac_result = ac_model.fit_predict(gower_oe_weighted_df)\n",
    "print(f\"Silhouette Score : {silhouette_score(gower_oe_weighted_df, ac_result)}\")\n",
    "print(f\"Calinski Score : {calinski_harabasz_score(gower_oe_weighted_df, ac_result)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations (n_clusters = 2 | linkage = complete)\n",
    "Categoric Variables:\n",
    "- `Resign Status` - 1\n",
    "\n",
    "Numeric Variables:\n",
    "- Snake Plot\n",
    "    - `Age` - 1\n",
    "    - `BusinessTravel` - 1\n",
    "    - `Salary` - 1\n",
    "    - `Length of Service` - 1\n",
    "- Heat Map\n",
    "    - `BusinessTravel` - 1\n",
    "    - `Salary` - 1\n",
    "    - `Length of Service` - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations (n_clusters = 3 | linakge = average)\n",
    "Categoric Variables:\n",
    "- `Resign Status` - 2\n",
    "- `Sales` - 1\n",
    "- `R&D` - 1\n",
    "\n",
    "Numeric Variables:\n",
    "- Snake Plot\n",
    "    - `Age` - 1\n",
    "    - `BusinessTravel` - 1\n",
    "    - `Distance Between Company and Home` - 1\n",
    "    - `Education` - 1\n",
    "    - `Salary` - 1\n",
    "    - `Performance Rating` - 1\n",
    "    - `Length of Service` - 1\n",
    "\n",
    "Problem:\n",
    "- Label 1 only has 8 staff clustered, which might heavily skew the insights of the cluster.\n",
    "- Label 1 are all outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    'Gender_Female',\n",
    "    'Job Function_Human Resources',\n",
    "    'Job Function_Research & Development',\n",
    "    'Job Function_Sales',\n",
    "    'MaritalStatus_Divorced',\n",
    "    'MaritalStatus_Married',\n",
    "    'MaritalStatus_Single',\n",
    "    'Resign Status_Yes'\n",
    "]\n",
    "numeric = [\n",
    "    'Age',\n",
    "    'BusinessTravel',\n",
    "    'Distance Between Company and Home (KM)',\n",
    "    'Education (1 is lowest, 5 is highest)',\n",
    "    'Job Satisfaction (1 is lowest, 4 is highest)',\n",
    "    'Salary ($)',\n",
    "    'Performance Rating (1 is lowest, 4 is highest)',\n",
    "    'Work Life Balance (1 is worst, 4 is best)',\n",
    "    'Length of Service (Years)'\n",
    "]\n",
    "\n",
    "labels = pd.DataFrame(ac_result, columns=[\"Labels\"])\n",
    "labelled_df = pd.concat([new_df_oe_weighted_nfirst, labels], axis=1)\n",
    "labelled_scaled_df = pd.concat([new_df_scaled_oe_weighted_nfirst, labels], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.DataFrame(ac_result, columns=[\"Labels\"])\n",
    "labelled_df = pd.concat([new_df_oe_weighted_nfirst, labels], axis=1)\n",
    "\n",
    "labelled_df_cat = labelled_df[categories + ['Labels']]\n",
    "cluster_count_proportion = labelled_df_cat.groupby(['Labels']).sum() / labelled_df_cat.groupby(['Labels']).count()\n",
    "population_count_proportion = labelled_df_cat.drop(columns=['Labels']).sum() / labelled_df_cat.drop(columns=['Labels']).count()\n",
    "relative_imp = cluster_count_proportion - population_count_proportion\n",
    "\n",
    "plt.figure(figsize=(25,8))\n",
    "plt.title('Relative Importance of attributes')\n",
    "sns.heatmap(data=relative_imp, annot=True, fmt='.2f', cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.DataFrame(ac_result, columns=[\"Labels\"])\n",
    "labelled_scaled_df = pd.concat([new_df_scaled_oe_weighted_nfirst, labels], axis=1)\n",
    "\n",
    "labelled_scaled_df_num = labelled_scaled_df[numeric + ['Labels']]\n",
    "labelled_scaled_df_num_melt = pd.melt(labelled_scaled_df_num, id_vars=['Labels'], value_vars=numeric, var_name='Attribute', value_name='Value')\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.title('Snake plot of standardized variables')\n",
    "sns.lineplot(x='Attribute', y='Value', hue='Labels', data=labelled_scaled_df_num_melt, palette='viridis')\n",
    "plt.xticks(rotation=90)\n",
    "plt.hlines(0, 0, len(numeric)-1, colors='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.DataFrame(ac_result, columns=[\"Labels\"])\n",
    "labelled_df = pd.concat([new_df_oe_weighted_nfirst, labels], axis=1)\n",
    "labelled_df_num = labelled_df[numeric + ['Labels']]\n",
    "cluster_avg = labelled_df_num.groupby(['Labels']).mean()\n",
    "population_avg = labelled_df_num.drop(columns=['Labels']).mean()\n",
    "relative_imp = cluster_avg / population_avg - 1\n",
    "\n",
    "plt.figure(figsize=(25,8))\n",
    "plt.title('Relative Importance of attributes')\n",
    "sns.heatmap(data=relative_imp, annot=True, fmt='.2f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.DataFrame(ac_result, columns=[\"Labels\"])\n",
    "labelled_df = pd.concat([new_df_oe_weighted_nfirst, labels], axis=1)\n",
    "\n",
    "x = 18\n",
    "fig, ax = plt.subplots(1,2, figsize=(20, 10))\n",
    "\n",
    "ax[0].title.set_text('Label 0')\n",
    "ax[1].title.set_text('Label 1')\n",
    "\n",
    "sns.histplot(data=labelled_df, x=labelled_df.columns[x], ax=ax[0])\n",
    "sns.histplot(data=labelled_df[labelled_df['Labels'] == 1], x=labelled_df.columns[x], ax=ax[1])\n",
    "ax[0].set_xlim([0, 45])\n",
    "ax[1].set_xlim([0, 45])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.DataFrame(ac_result, columns=[\"Labels\"])\n",
    "labelled_df = pd.concat([new_df_oe_weighted_nfirst, labels], axis=1)\n",
    "\n",
    "x =9\n",
    "fig, ax = plt.subplots(1,2, figsize=(20, 10))\n",
    "\n",
    "ax[0].title.set_text('Label 0')\n",
    "ax[1].title.set_text('Label 1')\n",
    "\n",
    "sns.countplot(data=labelled_df[labelled_df['Labels'] == 0], x=labelled_df.columns[x], ax=ax[0])\n",
    "sns.countplot(data=labelled_df[labelled_df['Labels'] == 1], x=labelled_df.columns[x], ax=ax[1])\n",
    "# ax[0].set_ylim([0, 1000])\n",
    "# ax[1].set_ylim([0, 1000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled[['Age']].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.DataFrame(ac_result, columns=[\"Labels\"])\n",
    "labelled_df = pd.concat([new_df_oe_weighted_nfirst, labels], axis=1)\n",
    "\n",
    "scaled = labelled_df.groupby(['Labels', 'BusinessTravel']).count() / labelled_df.groupby(['Labels']).count()\n",
    "scaled = scaled[['Age']].reset_index()\n",
    "\n",
    "x = 11\n",
    "fig, ax = plt.subplots(1,2, figsize=(20, 10))\n",
    "\n",
    "ax[0].title.set_text('Label 0')\n",
    "ax[1].title.set_text('Label 1')\n",
    "\n",
    "sns.barplot(data=scaled[scaled['Labels'] == 0], x='BusinessTravel', y='Age', ax=ax[0])\n",
    "sns.barplot(data=scaled[scaled['Labels'] == 1], x='BusinessTravel', y='Age', ax=ax[1])\n",
    "ax[0].set_ylabel(\"Count\")\n",
    "ax[1].set_ylabel(\"Count\")\n",
    "ax[0].set_ylim([0, 1])\n",
    "ax[1].set_ylim([0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, count = np.unique(ac_result, return_counts=True)\n",
    "dict(zip(u, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 7\n",
    "print(labelled_df.columns[col])\n",
    "\n",
    "for label in np.unique(ac_result):\n",
    "    u, count = np.unique(labelled_df[labelled_df['Labels'] == label][labelled_df.columns[col]], return_counts=True)\n",
    "    print(f\"{label} -\", dict(zip(u, count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ac_model.children_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_model = AgglomerativeClustering(n_clusters=6, compute_distances=True)\n",
    "pred_ac = ac_model.fit_predict(new_df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.DataFrame(ac_model.labels_, columns=['Labels'])\n",
    "ac_labeled_df = pd.concat([new_df_scaled, labels], axis=1)\n",
    "ac_labeled_df['Labels'] += 1\n",
    "ac_labeled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ac_model.distances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_model.children_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN\n",
    "\n",
    "Works best with noisy data with outliers and doesn't prefer spherical, globular, or elliptical clusters. It can cluster in any shape.\\\n",
    "\n",
    "How it works:\\\n",
    "`Epsilon`: It is considered radius around a given point\\\n",
    "`Minimum points`: It gives the minimum number of points that have to be present inside the Epsilon radius circle around a data point\\\n",
    "`Core point`: a data point has a number of points equal or more than the \"minimum points\" inside the radius of epsilon around the circle\\\n",
    "`Border point`: a data point does not have the minimum points required to make a core point but has at least one core point inside the epsilon radius around the points\\\n",
    "`Noise point`: a data point has no core points inside the epsilon radius around the point (DBSCAN rules out outliers)\n",
    "\n",
    "Not a great unsupervised model for clustering this dataset.\\\n",
    "\n",
    "Noisy samples are given the label -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_df = pd.DataFrame(new_df_scaled[['Salary ($)', 'Distance Between Company and Home (KM)', 'Length of Service (Years)']], columns=['Salary ($)', 'Distance Between Company and Home (KM)', 'Length of Service (Years)'])\n",
    "dbscan_model = DBSCAN(eps=0.1, metric='precomputed')\n",
    "pred_dbscan = dbscan_model.fit_predict(gower_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, count = np.unique(pred_dbscan, return_counts=True)\n",
    "dict(zip(u, count))\n",
    "## since all of the data is -1 for very small values of eps this means that DBSCAN is not a good model for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sil_arr = []\n",
    "x = np.linspace(0.0001, 1, 100)\n",
    "x_ = []\n",
    "for k in tqdm(x):\n",
    "    dbscan_model = DBSCAN(eps=k, metric='precomputed')\n",
    "    dbscan_result = dbscan_model.fit_predict(gower_df)\n",
    "    try:\n",
    "        sil_arr.append(silhouette_score(new_df_scaled, dbscan_result))\n",
    "        x_.append(k)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "plt.plot(x_, sil_arr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Mixtures\n",
    "Does not use a distance measure, but applies a probability distribution around the cluster centers to work out the likelihood that a data point belongs to a given cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm_model = GaussianMixture(n_components=2)\n",
    "gm_result = gm_model.fit_predict(gower_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(gm_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_score(gower_df, gm_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sil_arr = {'full': [], 'tied': [], 'diag': [], 'spherical': []}\n",
    "calinski_arr = {'full': [], 'tied': [], 'diag': [], 'spherical': []}\n",
    "aic_arr = {'full': [], 'tied': [], 'diag': [], 'spherical': []}\n",
    "bic_arr = {'full': [], 'tied': [], 'diag': [], 'spherical': []}\n",
    "x = np.arange(2, 15)\n",
    "for k in tqdm(x):\n",
    "    gm_model = GaussianMixture(n_components=k, covariance_type='full', random_state=42)\n",
    "    gm_result = gm_model.fit_predict(gower_oe_weighted_resigned_df)\n",
    "    sil_arr['full'].append(silhouette_score(gower_oe_weighted_resigned_df, gm_result))\n",
    "    calinski_arr['full'].append(calinski_harabasz_score(gower_oe_weighted_resigned_df, gm_result))\n",
    "    aic_arr['full'].append(gm_model.aic(gower_oe_weighted_resigned_df))\n",
    "    bic_arr['full'].append(gm_model.bic(gower_oe_weighted_resigned_df))\n",
    "\n",
    "    # gm_model = GaussianMixture(n_components=k, covariance_type='tied', random_state=42)\n",
    "    # gm_result = gm_model.fit_predict(gower_oe_weighted_resigned_df)\n",
    "    # sil_arr['tied'].append(silhouette_score(gower_oe_weighted_resigned_df, gm_result))\n",
    "    # calinski_arr['tied'].append(calinski_harabasz_score(gower_oe_weighted_resigned_df, gm_result))\n",
    "\n",
    "    gm_model = GaussianMixture(n_components=k, covariance_type='diag', random_state=42)\n",
    "    gm_result = gm_model.fit_predict(gower_oe_weighted_resigned_df)\n",
    "    sil_arr['diag'].append(silhouette_score(gower_oe_weighted_resigned_df, gm_result))\n",
    "    calinski_arr['diag'].append(calinski_harabasz_score(gower_oe_weighted_resigned_df, gm_result))\n",
    "    aic_arr['tied'].append(gm_model.aic(gower_oe_weighted_resigned_df))\n",
    "    bic_arr['tied'].append(gm_model.bic(gower_oe_weighted_resigned_df))\n",
    "\n",
    "    gm_model = GaussianMixture(n_components=k, covariance_type='spherical', random_state=42)\n",
    "    gm_result = gm_model.fit_predict(gower_oe_weighted_resigned_df)\n",
    "    sil_arr['spherical'].append(silhouette_score(gower_oe_weighted_resigned_df, gm_result))\n",
    "    calinski_arr['spherical'].append(calinski_harabasz_score(gower_oe_weighted_resigned_df, gm_result))\n",
    "    aic_arr['spherical'].append(gm_model.aic(gower_oe_weighted_resigned_df))\n",
    "    bic_arr['spherical'].append(gm_model.bic(gower_oe_weighted_resigned_df))\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(20, 20))\n",
    "\n",
    "ax[0,0].title.set_text('Silhouette Score')\n",
    "ax[0,0].plot(x, sil_arr['full'], label='full')\n",
    "# ax1.plot(x, sil_arr['tied'], label='tied')\n",
    "ax[0,0].plot(x, sil_arr['diag'], label='diag')\n",
    "ax[0,0].plot(x, sil_arr['spherical'], label='spherical')\n",
    "\n",
    "ax[0,1].title.set_text('Calinski Harabas Z Score')\n",
    "ax[0,1].plot(x, calinski_arr['full'], label='full')\n",
    "# ax2.plot(x, calinski_arr['tied'], label='tied')\n",
    "ax[0,1].plot(x, calinski_arr['diag'], label='diag')\n",
    "ax[0,1].plot(x, calinski_arr['spherical'], label='spherical')\n",
    "\n",
    "ax[1,0].title.set_text('AIC')\n",
    "ax[1,0].plot(x, aic_arr['full'], label='full')\n",
    "ax[1,0].plot(x, aic_arr['tied'], label='tied')\n",
    "# ax[1,0].plot(x, aic_arr['diag'], label='diag')\n",
    "ax[1,0].plot(x, aic_arr['spherical'], label='spherical')\n",
    "\n",
    "ax[1,1].title.set_text('BIC')\n",
    "ax[1,1].plot(x, bic_arr['full'], label='full')\n",
    "ax[1,1].plot(x, bic_arr['tied'], label='tied')\n",
    "# ax[1,1].plot(x, bic_arr['diag'], label='diag')\n",
    "ax[1,1].plot(x, bic_arr['spherical'], label='spherical')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sil_arr = {'full': [], 'tied': [], 'diag': [], 'spherical': []}\n",
    "calinski_arr = {'full': [], 'tied': [], 'diag': [], 'spherical': []}\n",
    "# aic_arr = {'full': [], 'tied': [], 'diag': [], 'spherical': []}\n",
    "# bic_arr = {'full': [], 'tied': [], 'diag': [], 'spherical': []}\n",
    "x = np.arange(2, 15)\n",
    "for k in tqdm(x):\n",
    "    gm_model = GaussianMixture(n_components=k, covariance_type='full', random_state=42)\n",
    "    gm_result = gm_model.fit_predict(gower_oe_weighted_df)\n",
    "    sil_arr['full'].append(silhouette_score(gower_oe_weighted_df, gm_result))\n",
    "    calinski_arr['full'].append(calinski_harabasz_score(gower_oe_weighted_df, gm_result))\n",
    "    # aic_arr['full'].append(gm_model.aic(gower_oe_weighted_df))\n",
    "    # bic_arr['full'].append(gm_model.bic(gower_oe_weighted_df))\n",
    "\n",
    "    # gm_model = GaussianMixture(n_components=k, covariance_type='tied', random_state=42)\n",
    "    # gm_result = gm_model.fit_predict(gower_oe_weighted_df)\n",
    "    # sil_arr['tied'].append(silhouette_score(gower_oe_weighted_df, gm_result))\n",
    "    # calinski_arr['tied'].append(calinski_harabasz_score(gower_oe_weighted_df, gm_result))\n",
    "\n",
    "    gm_model = GaussianMixture(n_components=k, covariance_type='diag', random_state=42)\n",
    "    gm_result = gm_model.fit_predict(gower_oe_weighted_df)\n",
    "    sil_arr['diag'].append(silhouette_score(gower_oe_weighted_df, gm_result))\n",
    "    calinski_arr['diag'].append(calinski_harabasz_score(gower_oe_weighted_df, gm_result))\n",
    "    # aic_arr['tied'].append(gm_model.aic(gower_oe_weighted_df))\n",
    "    # bic_arr['tied'].append(gm_model.bic(gower_oe_weighted_df))\n",
    "\n",
    "    gm_model = GaussianMixture(n_components=k, covariance_type='spherical', random_state=42)\n",
    "    gm_result = gm_model.fit_predict(gower_oe_weighted_df)\n",
    "    sil_arr['spherical'].append(silhouette_score(gower_oe_weighted_df, gm_result))\n",
    "    calinski_arr['spherical'].append(calinski_harabasz_score(gower_oe_weighted_df, gm_result))\n",
    "    # aic_arr['spherical'].append(gm_model.aic(gower_oe_weighted_df))\n",
    "    # bic_arr['spherical'].append(gm_model.bic(gower_oe_weighted_df))\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(20, 20))\n",
    "\n",
    "ax[0].title.set_text('Silhouette Score')\n",
    "ax[0].plot(x, sil_arr['full'], label='full')\n",
    "# ax1.plot(x, sil_arr['tied'], label='tied')\n",
    "ax[0].plot(x, sil_arr['diag'], label='diag')\n",
    "ax[0].plot(x, sil_arr['spherical'], label='spherical')\n",
    "\n",
    "ax[1].title.set_text('Calinski Harabas Z Score')\n",
    "ax[1].plot(x, calinski_arr['full'], label='full')\n",
    "# ax2.plot(x, calinski_arr['tied'], label='tied')\n",
    "ax[1].plot(x, calinski_arr['diag'], label='diag')\n",
    "ax[1].plot(x, calinski_arr['spherical'], label='spherical')\n",
    "\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "\n",
    "# ax[1,0].title.set_text('AIC')\n",
    "# ax[1,0].plot(x, aic_arr['full'], label='full')\n",
    "# ax[1,0].plot(x, aic_arr['tied'], label='tied')\n",
    "# ax[1,0].plot(x, aic_arr['diag'], label='diag')\n",
    "# ax[1,0].plot(x, aic_arr['spherical'], label='spherical')\n",
    "\n",
    "# ax[1,1].title.set_text('BIC')\n",
    "# ax[1,1].plot(x, bic_arr['full'], label='full')\n",
    "# ax[1,1].plot(x, bic_arr['tied'], label='tied')\n",
    "# ax[1,1].plot(x, bic_arr['diag'], label='diag')\n",
    "# ax[1,1].plot(x, bic_arr['spherical'], label='spherical')\n",
    "\n",
    "# plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm_model = GaussianMixture(n_components=4, random_state=42, covariance_type='full')\n",
    "gm_result = gm_model.fit_predict(gower_oe_weighted_df)\n",
    "\n",
    "# 0.07228, 67.78388 - 2 components\n",
    "# 0.156186, 225.3747 - 4 components, random state = 42\n",
    "# 0.15744, 200.63134 - 5 components\n",
    "\n",
    "print(f\"Silhouette Score : {silhouette_score(gower_oe_weighted_df, gm_result)}\")\n",
    "print(f\"Calinski Score : {calinski_harabasz_score(gower_oe_weighted_df, gm_result)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations (components = 4 | covariance type = full):\n",
    "\n",
    "Categoric Variables:\n",
    "- `Gender` - 1,3\n",
    "- `Job Function_Research&Development`\n",
    "- `Job Function_Sales`\n",
    "- `Resign Status` - 2\n",
    "\n",
    "Numeric Variables:\n",
    "- Snake Plot\n",
    "    - `Performance Rating` - 2\n",
    "    - `Distance Between Company and Home` - 2\n",
    "    - `Length of Service` - 2\n",
    "    - `Job Satisfaction` - 2\n",
    "- Heat Map\n",
    "    - `Distance Between Company and Home` - 2\n",
    "    - `Length of Service` - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations (components = 2 | covariance type = spherical)\n",
    "\n",
    "Categoric Variables:\n",
    "- None\n",
    "\n",
    "Numeric Variables:\n",
    "- Snake Plot\n",
    "    - `BusinessTravel`\n",
    "    - `Performance Rating`\n",
    "- Heat Map\n",
    "    - None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations (resigned | components = 2 | covariance type = full)\n",
    "\n",
    "Categoric Variables:\n",
    "- `Gender`\n",
    "\n",
    "Numeric Variables:\n",
    "- Snake Plot\n",
    "    - `Age` - 1\n",
    "    - `Length of Service` - 1\n",
    "- Heat Map\n",
    "    - `Length of Service` - 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    'Gender_Female',\n",
    "    'Job Function_Human Resources',\n",
    "    'Job Function_Research & Development',\n",
    "    'Job Function_Sales',\n",
    "    'MaritalStatus_Divorced',\n",
    "    'MaritalStatus_Married',\n",
    "    'MaritalStatus_Single',\n",
    "    'Resign Status_Yes'\n",
    "]\n",
    "numeric = [\n",
    "    'Age',\n",
    "    'BusinessTravel',\n",
    "    'Distance Between Company and Home (KM)',\n",
    "    'Education (1 is lowest, 5 is highest)',\n",
    "    'Job Satisfaction (1 is lowest, 4 is highest)',\n",
    "    'Salary ($)',\n",
    "    'Performance Rating (1 is lowest, 4 is highest)',\n",
    "    'Work Life Balance (1 is worst, 4 is best)',\n",
    "    'Length of Service (Years)'\n",
    "]\n",
    "\n",
    "labels = pd.DataFrame(gm_result, columns=[\"Labels\"])\n",
    "labelled_scaled_df = pd.concat([new_df_scaled_oe_weighted_nfirst, labels], axis=1)\n",
    "labelled_df = pd.concat([new_df_oe_weighted_nfirst, labels], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_df_cat = labelled_df[categories + ['Labels']]\n",
    "cluster_count_proportion = labelled_df_cat.groupby(['Labels']).sum() / labelled_df_cat.groupby(['Labels']).count()\n",
    "population_count_proportion = labelled_df_cat.drop(columns=['Labels']).sum() / labelled_df_cat.drop(columns=['Labels']).count()\n",
    "relative_imp = (cluster_count_proportion - population_count_proportion)\n",
    "\n",
    "plt.figure(figsize=(25,8))\n",
    "plt.title('Relative Importance of attributes')\n",
    "sns.heatmap(data=relative_imp, annot=True, fmt='.2f', cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_scaled_df_num = labelled_scaled_df[numeric + ['Labels']]\n",
    "labelled_scaled_df_num_melt = pd.melt(labelled_scaled_df_num, id_vars=['Labels'], value_vars=numeric, var_name='Attribute', value_name='Value')\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.title('Snake plot of standardized variables')\n",
    "sns.lineplot(x='Attribute', y='Value', hue='Labels', data=labelled_scaled_df_num_melt, palette='viridis')\n",
    "plt.xticks(rotation=90)\n",
    "plt.hlines(0, 0, len(numeric)-1, colors='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_df_num = labelled_df[numeric + ['Labels']]\n",
    "cluster_avg = labelled_df_num.groupby(['Labels']).mean()\n",
    "population_avg = labelled_df_num.drop(columns=['Labels']).mean()\n",
    "relative_imp = cluster_avg / population_avg - 1\n",
    "\n",
    "plt.figure(figsize=(25,8))\n",
    "plt.title('Relative Importance of attributes')\n",
    "sns.heatmap(data=relative_imp, annot=True, fmt='.2f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.DataFrame(gm_result, columns=['Labels'])\n",
    "# labelled_df = pd.concat([new_df_oe_weighted_nfirst[new_df_oe_weighted_nfirst['Resign Status_Yes'] == 1].reset_index(drop=True), labels], axis=1)\n",
    "labelled_df = pd.concat([new_df_oe_weighted_nfirst, labels], axis=1)\n",
    "\n",
    "x, y = 15, 13\n",
    "fig, ax = plt.subplots(2,2, figsize=(20, 20))\n",
    "\n",
    "ax[0,0].title.set_text('Label 0')\n",
    "ax[0,1].title.set_text('Label 1')\n",
    "ax[1,0].title.set_text('Label 2')\n",
    "ax[1,1].title.set_text('Label 3')\n",
    "# ax[2,0].title.set_text('Label 4')\n",
    "\n",
    "sns.kdeplot(data=labelled_df[labelled_df['Labels'] == 0], x=labelled_df.columns[x], y=labelled_df.columns[y], palette='viridis', ax=ax[0,0])\n",
    "sns.kdeplot(data=labelled_df[labelled_df['Labels'] == 1], x=labelled_df.columns[x], y=labelled_df.columns[y], palette='viridis', ax=ax[0,1])\n",
    "sns.kdeplot(data=labelled_df[labelled_df['Labels'] == 2], x=labelled_df.columns[x], y=labelled_df.columns[y], palette='viridis', ax=ax[1,0])\n",
    "sns.kdeplot(data=labelled_df[labelled_df['Labels'] == 3], x=labelled_df.columns[x], y=labelled_df.columns[y], palette='viridis', ax=ax[1,1])\n",
    "# sns.kdeplot(data=labelled_df[labelled_df['Labels'] == 4], x=labelled_df.columns[x], y=labelled_df.columns[y], palette='viridis', ax=ax[2,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 12\n",
    "fig, ax = plt.subplots(2,2, figsize=(20, 20))\n",
    "\n",
    "ax[0,0].title.set_text('Label 0')\n",
    "ax[0,1].title.set_text('Label 1')\n",
    "ax[1,0].title.set_text('Label 2')\n",
    "ax[1,1].title.set_text('Label 3')\n",
    "\n",
    "sns.histplot(data=labelled_df[labelled_df['Labels'] == 0], x=labelled_df.columns[x], ax=ax[0,0])\n",
    "sns.histplot(data=labelled_df[labelled_df['Labels'] == 1], x=labelled_df.columns[x], ax=ax[0,1])\n",
    "sns.histplot(data=labelled_df[labelled_df['Labels'] == 2], x=labelled_df.columns[x], ax=ax[1,0])\n",
    "sns.histplot(data=labelled_df[labelled_df['Labels'] == 3], x=labelled_df.columns[x], ax=ax[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 4\n",
    "fig, ax = plt.subplots(2,2, figsize=(20, 20))\n",
    "\n",
    "ax[0,0].title.set_text('Label 0')\n",
    "ax[0,1].title.set_text('Label 1')\n",
    "ax[1,0].title.set_text('Label 2')\n",
    "ax[1,1].title.set_text('Label 3')\n",
    "\n",
    "sns.countplot(data=labelled_df[labelled_df['Labels'] == 0], x=labelled_df.columns[x], ax=ax[0,0])\n",
    "sns.countplot(data=labelled_df[labelled_df['Labels'] == 1], x=labelled_df.columns[x], ax=ax[0,1])\n",
    "sns.countplot(data=labelled_df[labelled_df['Labels'] == 2], x=labelled_df.columns[x], ax=ax[1,0])\n",
    "sns.countplot(data=labelled_df[labelled_df['Labels'] == 3], x=labelled_df.columns[x], ax=ax[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.DataFrame(gm_result, columns=['Labels'])\n",
    "labelled_df = pd.concat([new_df_oe_weighted_nfirst[new_df_oe_weighted_nfirst['Resign Status_Yes'] == 1].reset_index(drop=True), labels], axis=1)\n",
    "\n",
    "x, y = 15,18\n",
    "fig, ax = plt.subplots(2,2, figsize=(20, 20))\n",
    "\n",
    "ax[0,0].title.set_text('Label 0')\n",
    "ax[0,1].title.set_text('Label 1')\n",
    "ax[1,0].title.set_text('Label 2')\n",
    "ax[1,1].title.set_text('Label 3')\n",
    "# ax[2,0].title.set_text('Label 4')\n",
    "\n",
    "ax[0,0].scatter(labelled_df[labelled_df['Labels'] == 0].iloc[:, x], labelled_df[labelled_df['Labels'] == 0].iloc[:, y])\n",
    "ax[0,0].set_xlabel(labelled_df[labelled_df['Labels'] == 0].columns[x])\n",
    "ax[0,0].set_ylabel(labelled_df[labelled_df['Labels'] == 0].columns[y])\n",
    "\n",
    "ax[0,1].scatter(labelled_df[labelled_df['Labels'] == 1].iloc[:, x], labelled_df[labelled_df['Labels'] == 1].iloc[:, y])\n",
    "ax[0,1].set_xlabel(labelled_df[labelled_df['Labels'] == 1].columns[x])\n",
    "ax[0,1].set_ylabel(labelled_df[labelled_df['Labels'] == 1].columns[y])\n",
    "\n",
    "ax[1,0].scatter(labelled_df[labelled_df['Labels'] == 2].iloc[:, x], labelled_df[labelled_df['Labels'] == 2].iloc[:, y])\n",
    "ax[1,0].set_xlabel(labelled_df[labelled_df['Labels'] == 2].columns[x])\n",
    "ax[1,0].set_ylabel(labelled_df[labelled_df['Labels'] == 2].columns[y])\n",
    "\n",
    "ax[1,1].scatter(labelled_df[labelled_df['Labels'] == 3].iloc[:, x], labelled_df[labelled_df['Labels'] == 3].iloc[:, y])\n",
    "ax[1,1].set_xlabel(labelled_df[labelled_df['Labels'] == 3].columns[x])\n",
    "ax[1,1].set_ylabel(labelled_df[labelled_df['Labels'] == 3].columns[y])\n",
    "\n",
    "# ax[2,0].scatter(labelled_df[labelled_df['Labels'] == 4].iloc[:, x], labelled_df[labelled_df['Labels'] == 4].iloc[:, y])\n",
    "# ax[2,0].set_xlabel(labelled_df[labelled_df['Labels'] == 4].columns[x])\n",
    "# ax[2,0].set_ylabel(labelled_df[labelled_df['Labels'] == 4].columns[y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, count = np.unique(gm_result, return_counts=True)\n",
    "dict(zip(u, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gm_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(oe_weighted_df[oe_weighted_df['Resign Status'] == 'Yes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 13\n",
    "print(labelled_df.columns[col])\n",
    "\n",
    "for label in np.unique(gm_result):\n",
    "    u, count = np.unique(labelled_df[labelled_df['Labels'] == label][labelled_df.columns[col]], return_counts=True)\n",
    "    print(f\"{label} -\", dict(zip(u, count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(labelled_df[labelled_df['Labels'] == 3][labelled_df.columns[15]])\n",
    "# Salary std 0 - 3446.3785\n",
    "# Salary std 1 - 4705.7629\n",
    "# Salary std 2 - 6240.3782\n",
    "# Salary std 3 - 4579.8788"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    avg = np.mean(labelled_df[labelled_df['Labels'] == i][labelled_df.columns[15]])\n",
    "    std = np.std(labelled_df[labelled_df['Labels'] == i][labelled_df.columns[15]])\n",
    "    print(f\"Label {i} : Mean - {avg} | Std - {std}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d5babd99a4df02ec48601645cc9db139875b0384e29e5b3b3c9a97142b6b19c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
